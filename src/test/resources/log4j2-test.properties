# Set to debug or trace if log4j initialization is failing
status = warn

rootLogger.level=info
rootLogger.appenderRef.stdout.ref=consoleLogger

appender.console.type=Console
appender.console.name=consoleLogger
appender.console.target=SYSTEM_OUT
appender.console.layout.type=PatternLayout
appender.console.layout.pattern=%d{yy-MM-dd HH:mm:ss} %p %c{1}: %m%n%ex

logger.pp.name=com.company
logger.pp.level=debug
logger.pp.additivity=false
logger.pp.appenderRef.console.ref=consoleLogger
logger.pp1.name=com.company2
logger.pp1.level=debug
logger.pp1.additivity=false
logger.pp1.appenderRef.console.ref=consoleLogger

# Settings to quiet third party logs that are too verbose com.amazonaws.services.s3
logger.jetty.name=org.sparkproject.jetty
logger.jetty.level=warn
logger.jetty2.name=org.sparkproject.jetty.util.component.AbstractLifeCycle
logger.jetty2.level=error
logger.repl1.name=org.apache.spark.repl.SparkIMain$exprTyper
logger.repl1.level=info
logger.repl2.name=org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
logger.repl2.level=info




# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
logger.repl.name=org.apache.spark.repl.Main
logger.repl.level=warn

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs
# in SparkSQL with Hive support
logger.metastore.name=org.apache.hadoop.hive.metastore.RetryingHMSHandler
logger.metastore.level=fatal
logger.hive_functionregistry.name=org.apache.hadoop.hive.ql.exec.FunctionRegistry
logger.hive_functionregistry.level=error

# Parquet related logging
logger.parquet.name=org.apache.parquet.CorruptStatistics
logger.parquet.level=error
logger.parquet2.name=parquet.CorruptStatistics
logger.parquet2.level=error
